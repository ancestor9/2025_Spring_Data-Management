{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# .env 파일에서 환경변수 불러오기\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Hugging Face 로그인\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6575da9668ea3e91dc9e2ab4', 'name': 'ancestor9', 'fullname': 'chosanggoo', 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6575da9668ea3e91dc9e2ab4/of9TD5GbKzd0IWOxTKrVq.png', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'exaone-token', 'role': 'fineGrained', 'createdAt': '2025-05-12T13:26:21.913Z', 'fineGrained': {'canReadGatedRepos': True, 'global': [], 'scoped': [{'entity': {'_id': '6575da9668ea3e91dc9e2ab4', 'type': 'user', 'name': 'ancestor9'}, 'permissions': ['repo.content.read']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(hf_token)\n",
    "print(whoami())  # 사용자 정보 출력되어야 정상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\" # \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8461e8093294dc0bfa4acfb99362531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158109eef49f4b69bc5519ebf66118f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_exaone.py:   0%|          | 0.00/9.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4da206561b4a23b16350e696de2d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_exaone.py:   0%|          | 0.00/63.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f54961e60643dd8865acfef2fd4e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/22.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf45cb0dd8f84f738f7b267fbc7c6032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f8e4d3cb53417f95961dfd7314b30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b77eabf0cec4ba08a18a7571a35370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baab7589a4f49d7a70ecc0ad627c096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b97574c50934754be91b2682e812ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    # token=hf_token,  # 필요한 경우 언주석\n",
    "    torch_dtype=torch.float16,  # 메모리 효율성 개선\n",
    "    device_map=\"auto\",  # 자동 디바이스 매핑\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b7d7fb22140f8a4a5abc71739ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/70.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8bb811405540a6965255b24a5ea0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885237a870054e34893ef1ed7e92d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648fb5f976ea4165860b843c55b04e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02a529939ad4b3f96fb63417e83baa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]스스로를 자랑해 봐\n",
      "[|assistant|]저는 LG AI Research에서 개발된 EXAONE 모델로서, 뛰어난 자연어 처리 능력과 방대한 양의 데이터를 기반으로 한 학습을 통해 사용자 여러분께 정확하고 창의적인 응답을 제공합니다. 지속적인 업데이트와 연구를 통해 인간과의 상호작용을 더욱 향상시키고 있으며, 다양한 질문에 대해 신속하고 신뢰할 수 있는 답변을 드리는 데 최선을 다하고 있습니다. 이러한 기술적 역량은 고객 서비스 향상과 혁신적인 솔루션 개발에 기여하고 있습니다.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "# Choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"  # English example\n",
    "prompt = \"스스로를 자랑해 봐\"       # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,           # 확률적 샘플링 \n",
    "    temperature=0.7,          # 온도 조절\n",
    "    top_p=0.9,                # 누적 확률 필터링\n",
    "    num_return_sequences=1,   # 단일 시퀀스 반환\n",
    "    use_cache=True            # 캐싱 활성화\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Tutor는 LG AI Research에서 개발한 인공지능 기반 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 지원 도구입니다. 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. **맞춤형 학습 경로 제공**: 사용자의 학습 목표와 능력 수준을 분석하여 개인 맞춤형 학습 경로를 제시합니다.\n",
      "2. **다양한 학습 자료**: 텍스트, 이미지, 오디오 등 다양한 형식의 학습 자료를 제공하여 유연하고 효과적인 학습 환경을 조성합니다.\n",
      "3. **실시간 피드백**: 학습 과정에서 실시간으로 피드백을 제공하여 학습 효과를 극대화합니다.\n",
      "4. **지속적인 업데이트**: 최신 연구 동향"
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "import torch\n",
    "import threading\n",
    "\n",
    "# 메시지 구성\n",
    "prompt = \"AI Tutor에 대해 설명해봐\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 스트리머 설정\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# generate 함수 비동기로 실행\n",
    "thread = threading.Thread(target=model.generate, kwargs={\n",
    "    \"input_ids\": input_ids,\n",
    "    \"streamer\": streamer,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "})\n",
    "thread.start()\n",
    "\n",
    "# 스트리밍 출력\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://0955a37f4494c53d97.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0955a37f4494c53d97.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "# 스트리밍 챗 함수\n",
    "def stream_chat(user_input):\n",
    "    global chat_history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"}] + chat_history\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"use_cache\": True,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    output_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        output_text += new_text\n",
    "        yield output_text\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 💡 EXAONE 스트리밍 챗\\n아래에 질문을 입력하세요:\")\n",
    "    with gr.Row():\n",
    "        txt_input = gr.Textbox(placeholder=\"메시지를 입력하세요...\", label=\"User Input\", lines=2)\n",
    "    with gr.Row():\n",
    "        output_text = gr.Textbox(label=\"AI 응답\", lines=10)\n",
    "\n",
    "    submit_btn = gr.Button(\"💬 전송\")\n",
    "    clear_btn = gr.Button(\"🧹 초기화\")\n",
    "\n",
    "    def clear_history():\n",
    "        global chat_history\n",
    "        chat_history = []\n",
    "        return \"\", \"\"\n",
    "\n",
    "    submit_btn.click(fn=stream_chat, inputs=txt_input, outputs=output_text)\n",
    "    clear_btn.click(fn=clear_history, outputs=[txt_input, output_text])\n",
    "\n",
    "# 외부 공유 링크 활성화\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
