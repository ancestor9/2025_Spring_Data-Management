{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Hugging Face ë¡œê·¸ì¸\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6575da9668ea3e91dc9e2ab4', 'name': 'ancestor9', 'fullname': 'chosanggoo', 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6575da9668ea3e91dc9e2ab4/of9TD5GbKzd0IWOxTKrVq.png', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'exaone-token', 'role': 'fineGrained', 'createdAt': '2025-05-12T13:26:21.913Z', 'fineGrained': {'canReadGatedRepos': True, 'global': [], 'scoped': [{'entity': {'_id': '6575da9668ea3e91dc9e2ab4', 'type': 'user', 'name': 'ancestor9'}, 'permissions': ['repo.content.read']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(hf_token)\n",
    "print(whoami())  # ì‚¬ìš©ì ì •ë³´ ì¶œë ¥ë˜ì–´ì•¼ ì •ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\" # \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8461e8093294dc0bfa4acfb99362531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158109eef49f4b69bc5519ebf66118f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_exaone.py:   0%|          | 0.00/9.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4da206561b4a23b16350e696de2d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_exaone.py:   0%|          | 0.00/63.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f54961e60643dd8865acfef2fd4e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/22.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf45cb0dd8f84f738f7b267fbc7c6032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f8e4d3cb53417f95961dfd7314b30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b77eabf0cec4ba08a18a7571a35370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baab7589a4f49d7a70ecc0ad627c096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b97574c50934754be91b2682e812ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    # token=hf_token,  # í•„ìš”í•œ ê²½ìš° ì–¸ì£¼ì„\n",
    "    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ \n",
    "    device_map=\"auto\",  # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b7d7fb22140f8a4a5abc71739ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/70.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8bb811405540a6965255b24a5ea0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885237a870054e34893ef1ed7e92d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648fb5f976ea4165860b843c55b04e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02a529939ad4b3f96fb63417e83baa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]ìŠ¤ìŠ¤ë¡œë¥¼ ìë‘í•´ ë´\n",
      "[|assistant|]ì €ëŠ” LG AI Researchì—ì„œ ê°œë°œëœ EXAONE ëª¨ë¸ë¡œì„œ, ë›°ì–´ë‚œ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í•™ìŠµì„ í†µí•´ ì‚¬ìš©ì ì—¬ëŸ¬ë¶„ê»˜ ì •í™•í•˜ê³  ì°½ì˜ì ì¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ì™€ ì—°êµ¬ë¥¼ í†µí•´ ì¸ê°„ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì‹ ì†í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ë“œë¦¬ëŠ” ë° ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì  ì—­ëŸ‰ì€ ê³ ê° ì„œë¹„ìŠ¤ í–¥ìƒê³¼ í˜ì‹ ì ì¸ ì†”ë£¨ì…˜ ê°œë°œì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "# Choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"  # English example\n",
    "prompt = \"ìŠ¤ìŠ¤ë¡œë¥¼ ìë‘í•´ ë´\"       # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,           # í™•ë¥ ì  ìƒ˜í”Œë§ \n",
    "    temperature=0.7,          # ì˜¨ë„ ì¡°ì ˆ\n",
    "    top_p=0.9,                # ëˆ„ì  í™•ë¥  í•„í„°ë§\n",
    "    num_return_sequences=1,   # ë‹¨ì¼ ì‹œí€€ìŠ¤ ë°˜í™˜\n",
    "    use_cache=True            # ìºì‹± í™œì„±í™”\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI TutorëŠ” LG AI Researchì—ì„œ ê°œë°œí•œ ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì§€ì› ë„êµ¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë§ì¶¤í˜• í•™ìŠµ ê²½ë¡œ ì œê³µ**: ì‚¬ìš©ìì˜ í•™ìŠµ ëª©í‘œì™€ ëŠ¥ë ¥ ìˆ˜ì¤€ì„ ë¶„ì„í•˜ì—¬ ê°œì¸ ë§ì¶¤í˜• í•™ìŠµ ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "2. **ë‹¤ì–‘í•œ í•™ìŠµ ìë£Œ**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ í•™ìŠµ ìë£Œë¥¼ ì œê³µí•˜ì—¬ ìœ ì—°í•˜ê³  íš¨ê³¼ì ì¸ í•™ìŠµ í™˜ê²½ì„ ì¡°ì„±í•©ë‹ˆë‹¤.\n",
      "3. **ì‹¤ì‹œê°„ í”¼ë“œë°±**: í•™ìŠµ ê³¼ì •ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ í•™ìŠµ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
      "4. **ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸**: ìµœì‹  ì—°êµ¬ ë™í–¥"
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "import torch\n",
    "import threading\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "prompt = \"AI Tutorì— ëŒ€í•´ ì„¤ëª…í•´ë´\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# í† í°í™”\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì •\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# generate í•¨ìˆ˜ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰\n",
    "thread = threading.Thread(target=model.generate, kwargs={\n",
    "    \"input_ids\": input_ids,\n",
    "    \"streamer\": streamer,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "})\n",
    "thread.start()\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://0955a37f4494c53d97.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0955a37f4494c53d97.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° ì±— í•¨ìˆ˜\n",
    "def stream_chat(user_input):\n",
    "    global chat_history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"}] + chat_history\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"use_cache\": True,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    output_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        output_text += new_text\n",
    "        yield output_text\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ’¡ EXAONE ìŠ¤íŠ¸ë¦¬ë° ì±—\\nì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:\")\n",
    "    with gr.Row():\n",
    "        txt_input = gr.Textbox(placeholder=\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\", label=\"User Input\", lines=2)\n",
    "    with gr.Row():\n",
    "        output_text = gr.Textbox(label=\"AI ì‘ë‹µ\", lines=10)\n",
    "\n",
    "    submit_btn = gr.Button(\"ğŸ’¬ ì „ì†¡\")\n",
    "    clear_btn = gr.Button(\"ğŸ§¹ ì´ˆê¸°í™”\")\n",
    "\n",
    "    def clear_history():\n",
    "        global chat_history\n",
    "        chat_history = []\n",
    "        return \"\", \"\"\n",
    "\n",
    "    submit_btn.click(fn=stream_chat, inputs=txt_input, outputs=output_text)\n",
    "    clear_btn.click(fn=clear_history, outputs=[txt_input, output_text])\n",
    "\n",
    "# ì™¸ë¶€ ê³µìœ  ë§í¬ í™œì„±í™”\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
