{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/2025_Spring_Data-Management/blob/main/week_05/Text_Representation_and_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYhh4devHeN"
      },
      "source": [
        "# í…ìŠ¤íŠ¸ í‘œí˜„ ê¸°ë²•ê³¼ ì„ë² ë”©\n",
        "# **Data Representation**\n",
        "\n",
        "## 1. Tabular data\n",
        "<img src='http://jalammar.github.io/images/pandas-intro/0%20excel-to-pandas.png'>\n",
        "\n",
        "## 2. Audio and Timeseries data\n",
        "<img src= 'http://jalammar.github.io/images/numpy/numpy-audio.png'>\n",
        "\n",
        "## 3. Image data\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-grayscale-image.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-color-image.png'>\n",
        "\n",
        "## <font color='orange'>**4. Text data**\n",
        "- **ì•„ë˜ ê·¸ë¦¼ì„ ì´í•´í•˜ì—¬ì•¼ í•œë‹¤.**\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-embeddings.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-bert-shape.png'>\n",
        "\n",
        "\n",
        "## <font color='orange'>**ëª©ì†Œë¦¬, ì£¼ì‹ê°€ê²©, ê·¸ë¦¼, ë™ì˜ìƒ, ì–¸ì–´ëŠ” ëª¨ë‘ ìˆœì„œ(Order)ê°€ ìˆë‹¤.**"
      ],
      "id": "cUYhh4devHeN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33pJFFSjvHeQ"
      },
      "source": [
        "## ğŸ¯ ê°•ì˜ ëª©í‘œ\n",
        "- ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•ì˜ ì—­ì‚¬ì™€ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤.\n",
        "- Bag of Words, TF-IDF, Word Embedding ê¸°ë²•ì„ ì‹¤ìŠµí•œë‹¤.\n",
        "- Word2Vecê³¼ ê°™ì€ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”©ì„ ì ìš©í•´ ë³¸ë‹¤."
      ],
      "id": "33pJFFSjvHeQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSCZdepMvHeR"
      },
      "source": [
        "## ğŸ“˜ ì´ë¡  ê°•ì˜\n",
        "### **1. í…ìŠ¤íŠ¸ í‘œí˜„(Representation of Text)ì˜ í•„ìš”ì„±**\n",
        "- ì»´í“¨í„°ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì´í•´í•´ì•¼ í•¨\n",
        "- ìì—°ì–´ëŠ” ë¹„ì •í˜• ë°ì´í„° â†’ ìˆ˜ì¹˜í™” í•„ìš”\n",
        "\n",
        "### **2. í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ì‹**\n",
        "#### 2.1 One-hot Encoding\n",
        "- ê° ë‹¨ì–´ë¥¼ ê³ ìœ  ì¸ë±ìŠ¤ë¡œ ë³€í™˜ í›„, ê·¸ ì¸ë±ìŠ¤ë§Œ 1ì¸ ë²¡í„°ë¡œ í‘œí˜„\n",
        "- ë‹¨ì : í¬ì†Œì„±, ë‹¨ì–´ ê°„ ì˜ë¯¸ ê´€ê³„ ì—†ìŒ\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- ë¬¸ì„œë³„ ë‹¨ì–´ ì¶œí˜„ ë¹ˆë„ ë²¡í„°\n",
        "- ì¥ì : ë‹¨ìˆœí•˜ê³  ë¹ ë¦„\n",
        "- ë‹¨ì : ë¬¸ë§¥ ì •ë³´ ì†ì‹¤\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram ëª¨ë¸\n",
        "- ì—°ì†ëœ Nê°œì˜ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ íŠ¹ì§•ìœ¼ë¡œ ê°„ì£¼\n",
        "- ì˜ˆ: bigram(\"I love NLP\") â†’ [\"I love\", \"love NLP\"]\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•œ ë²¡í„°\n",
        "- TF: ë¬¸ì„œ ë‚´ ë¹ˆë„ / IDF: ì „ì²´ ë¬¸ì„œì—ì„œì˜ í¬ê·€ì„±\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- ì˜ë¯¸ ê¸°ë°˜ ë¶„ì‚° í‘œí˜„ (Distributed Representation)\n",
        "- Word2Vec, GloVe, FastText ë“±\n",
        "- ë‹¨ì–´ ê°„ ìœ ì‚¬ë„, ì˜ë¯¸ ì¶”ë¡  ê°€ëŠ¥\n",
        "- CBOW / Skip-gram\n",
        "\n",
        "#### 2.6 ì‚¬ì „í•™ìŠµ ì„ë² ë”©\n",
        "- Gensim / HuggingFace Transformers ì‚¬ìš© ê°€ëŠ¥\n",
        "- ì˜ˆ: word2vec-google-news-300"
      ],
      "id": "lSCZdepMvHeR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxdzGGhUvHeS"
      },
      "source": [
        "## ğŸ’» **ì‹¤ìŠµ**\n",
        "\n",
        "#### 2.0 ì •ìˆ˜ì¸ì½”ë”©(Integer Encoding)\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `Tokenizer`\n",
        "- ì˜ˆë¥¼ ë“¤ì–´ : ë‹¨ì–´ì— ì •ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•œ ë‹¨ì–´ ì§‘í•©(vocabulary)ì„ ë§Œë“¤ê³ , ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì°¨ë¡€ë¡œ ë‚®ì€ ìˆ«ìë¶€í„° ì •ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•\n"
      ],
      "id": "RxdzGGhUvHeS"
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = '''\n",
        "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\n",
        "'''"
      ],
      "metadata": {
        "id": "i3jAB2Uj524_"
      },
      "id": "i3jAB2Uj524_",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # ë¬¸ì¥ì„ êµ¬ë¶„í•˜ê±°ë‚˜ ë‹¨ì–´ë¡œ ìª¼ê°¤ ë•Œ í•„ìš”í•œ pre-trained ëª¨ë¸\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dUYxkd6QXc",
        "outputId": "636cfdb2-f460-4b7f-fff2-c3b12b2af5d4"
      },
      "id": "36dUYxkd6QXc",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¬¸ì¥ í† í°í™”\n",
        "sentences = sent_tokenize(raw_text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZvs3umR6YBv",
        "outputId": "8981f2d8-5817-4781-8da3-e537ba928203"
      },
      "id": "mZvs3umR6YBv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nA barber is a person.',\n",
              " 'a barber is good person.',\n",
              " 'a barber is huge person.',\n",
              " 'he Knew A Secret!',\n",
              " 'The Secret He Kept is huge secret.',\n",
              " 'Huge secret.',\n",
              " 'His barber kept his word.',\n",
              " 'a barber kept his word.',\n",
              " 'His barber kept his secret.',\n",
              " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
              " 'the barber went up a huge mountain.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVb_2eul68fw",
        "outputId": "8a58d410-7578-4495-ff69-1206d4a059df"
      },
      "id": "OVb_2eul68fw",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e_AYFth6lRG",
        "outputId": "56b8865f-e006-4d5f-9b6b-46002f56c8ad"
      },
      "id": "3e_AYFth6lRG",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A barber is a person.\n",
            "a barber is good person.\n",
            "a barber is huge person.\n",
            "he Knew A Secret!\n",
            "The Secret He Kept is huge secret.\n",
            "Huge secret.\n",
            "His barber kept his word.\n",
            "a barber kept his word.\n",
            "His barber kept his secret.\n",
            "But keeping and keeping such a huge secret to himself was driving the barber crazy.\n",
            "the barber went up a huge mountain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0nLGE_o7G7s",
        "outputId": "644e2f8f-9ee0-4679-9fb6-f1a80408ba98"
      },
      "id": "_0nLGE_o7G7s",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # ë‹¨ì–´ í† í°í™”\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence:\n",
        "        word = word.lower() # ëª¨ë“  ë‹¨ì–´ë¥¼ ì†Œë¬¸ìí™”í•˜ì—¬ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¤„ì¸ë‹¤.\n",
        "        if word not in stop_words: # ë‹¨ì–´ í† í°í™” ëœ ê²°ê³¼ì— ëŒ€í•´ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œë‹¤.\n",
        "            if len(word) > 2: # ë‹¨ì–´ ê¸¸ì´ê°€ 2ì´í•˜ì¸ ê²½ìš°ì— ëŒ€í•˜ì—¬ ì¶”ê°€ë¡œ ë‹¨ì–´ë¥¼ ì œê±°í•œë‹¤.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0\n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZHeCqyE8qbF",
        "outputId": "6dabc246-c5b2-4a31-e494-e65ac2316554"
      },
      "id": "LZHeCqyE8qbF",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë‹¨ì–´ ì§‘í•© :',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZmssNVS7lPu",
        "outputId": "ea6e31cf-b951-4016-91e3-2231cc42622c"
      },
      "id": "CZmssNVS7lPu",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•© : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "vocab_sorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aMP3RFW7qG3",
        "outputId": "4e3098eb-2391-4a60-a9b3-7bf613be6f4a"
      },
      "id": "1aMP3RFW7qG3",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8),\n",
              " ('secret', 6),\n",
              " ('huge', 5),\n",
              " ('kept', 4),\n",
              " ('person', 3),\n",
              " ('word', 2),\n",
              " ('keeping', 2),\n",
              " ('good', 1),\n",
              " ('knew', 1),\n",
              " ('driving', 1),\n",
              " ('crazy', 1),\n",
              " ('went', 1),\n",
              " ('mountain', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ì¼ìˆ˜ë¡ ë‚®ì€ ì •ìˆ˜ë¥¼ 1ë¶€í„° ë¶€ì—¬\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # ë¹ˆë„ìˆ˜ê°€ ì‘ì€ ë‹¨ì–´ëŠ” ì œì™¸.\n",
        "        i = i + 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Plou9y86iV",
        "outputId": "41ae46e1-1fc9-4e9d-f56b-d953f2498bf3"
      },
      "id": "h-Plou9y86iV",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•˜ë‹¤ë³´ë©´, í…ìŠ¤íŠ¸ ë°ì´í„°ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ê¸° ë³´ë‹¤ëŠ” ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ nê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ\n",
        "vocab_size = 5\n",
        "\n",
        "# ì¸ë±ìŠ¤ê°€ 5 ì´ˆê³¼ì¸ ë‹¨ì–´ ì œê±°\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# í•´ë‹¹ ë‹¨ì–´ì— ëŒ€í•œ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì‚­ì œ\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khblo_Xp9OQw",
        "outputId": "a54cdf36-5c6e-46d1-8c2d-1924be3b5238"
      },
      "id": "Khblo_Xp9OQw",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì´ ìƒê¸°ëŠ” ìƒí™©ì„ Out-Of-Vocabulary(ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´) 'OOV ë¬¸ì œ'\n",
        "# word_to_indexì— 'OOV'ë€ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ì¶”ê°€í•˜ê³ , ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì€ 'OOV'ì˜ ì¸ë±ìŠ¤ë¡œ ì¸ì½”ë”©"
      ],
      "metadata": {
        "id": "SOGqyKk--EXu"
      },
      "id": "SOGqyKk--EXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT0Ad9OM-EbD",
        "outputId": "56315130-2582-4170-8329-0e9e3c1ec01c"
      },
      "id": "tT0Ad9OM-EbD",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            # ë‹¨ì–´ ì§‘í•©ì— ìˆëŠ” ë‹¨ì–´ë¼ë©´ í•´ë‹¹ ë‹¨ì–´ì˜ ì •ìˆ˜ë¥¼ ë¦¬í„´.\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            # ë§Œì•½ ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ë¼ë©´ 'OOV'ì˜ ì •ìˆ˜ë¥¼ ë¦¬í„´.\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PBLu0Zb-Zrq",
        "outputId": "e6531030-de1a-4097-8053-0ccc34a9b543"
      },
      "id": "0PBLu0Zb-Zrq",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Counter**"
      ],
      "metadata": {
        "id": "AMB44tjg-kKM"
      },
      "id": "AMB44tjg-kKM"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "CdC-Fa6X-jP4"
      },
      "id": "CdC-Fa6X-jP4",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSPBzPPV-qb3",
        "outputId": "78cf91a9-6878-41a1-c9bd-7f12a8ae471c"
      },
      "id": "jSPBzPPV-qb3",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: preprocessed_sentencesë¥¼ í•˜ë‚˜ì˜ ë‹¨ì–´ì§‘í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of words\n",
        "all_words = [word for sentence in preprocessed_sentences for word in sentence]\n",
        "\n",
        "# Create a vocabulary using Counter\n",
        "vocab = Counter(all_words)\n",
        "\n",
        "vocab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6UcS_V-wrM",
        "outputId": "9412ca30-3951-4626-dd09-1bf96fe7951d"
      },
      "id": "i_6UcS_V-wrM",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'barber': 8,\n",
              "         'person': 3,\n",
              "         'good': 1,\n",
              "         'huge': 5,\n",
              "         'knew': 1,\n",
              "         'secret': 6,\n",
              "         'kept': 4,\n",
              "         'word': 2,\n",
              "         'keeping': 2,\n",
              "         'driving': 1,\n",
              "         'crazy': 1,\n",
              "         'went': 1,\n",
              "         'mountain': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# words = np.hstack(preprocessed_sentences)ìœ¼ë¡œë„ ìˆ˜í–‰ ê°€ëŠ¥.\n",
        "all_words = sum(preprocessed_sentences, [])\n",
        "Counter(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhb6Dvug-qd_",
        "outputId": "0a957140-56fe-41d5-f121-8962e217b12c"
      },
      "id": "Dhb6Dvug-qd_",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'barber': 8,\n",
              "         'person': 3,\n",
              "         'good': 1,\n",
              "         'huge': 5,\n",
              "         'knew': 1,\n",
              "         'secret': 6,\n",
              "         'kept': 4,\n",
              "         'word': 2,\n",
              "         'keeping': 2,\n",
              "         'driving': 1,\n",
              "         'crazy': 1,\n",
              "         'went': 1,\n",
              "         'mountain': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"barber\"]) # 'barber'ë¼ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¶œë ¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYWwI5Zf-qf9",
        "outputId": "5f87e03a-0458-4375-caef-fdd87a495e8f"
      },
      "id": "OYWwI5Zf-qf9",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzrdmZsx-qh_",
        "outputId": "48781cd9-349a-4ebd-df41-ef437f7f373b"
      },
      "id": "hzrdmZsx-qh_",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='red'> **í€´ì¦ˆ 1. ì´ìƒí•œ ë‚˜ë¼ì˜ ì•¨ë¦¬ìŠ¤ë¼ëŠ” ì†Œì„¤ì—ì„œ ì¶œí˜„í•˜ëŠ” ìƒìœ„ 10ìœ„ ë‹¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€?**"
      ],
      "metadata": {
        "id": "1egoGVtjBcQE"
      },
      "id": "1egoGVtjBcQE"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('gutenberg')\n",
        "# êµ¬í…ë² ë¥´í¬ ë§ë­‰ì¹˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ ëª©ë¡ ì¶œë ¥\n",
        "file_ids = gutenberg.fileids()\n",
        "file_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLrNAg0E1ppo",
        "outputId": "392ddec1-dd5d-43ad-c8dc-40bf25dded49"
      },
      "id": "HLrNAg0E1ppo",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = gutenberg.raw('austen-emma.txt')\n",
        "raw_text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "hTuYZRTt4ypE",
        "outputId": "900ddad7-aac6-4b7f-f057-9c59077da75b"
      },
      "id": "hTuYZRTt4ypE",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.\\n\\nSixteen years had Miss Taylor been in Mr. Woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of Emma.  Between _them_ it was more the intimacy\\nof sisters.  Even before Miss Taylor had ceased to hold the nominal\\noffice of governess, the mildness o\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: raw_textì—ì„œ ì¶œí˜„í•˜ëŠ” ë‹¨ì–´ ì‚¬ìš°ì´ 10ê°œì™€ ì¶œí˜„ë¹ˆë„ë¥¼ êµ¬í•´ì¤˜\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w.lower() for w in tokens if w.isalnum() and w.lower() not in stop_words]\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the top 10 most frequent words\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "top_10_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehNwTHnz4yyc",
        "outputId": "8ca3abb9-6993-42dc-aebb-ac24bfc588b7"
      },
      "id": "ehNwTHnz4yyc",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('emma', 860),\n",
              " ('could', 836),\n",
              " ('would', 818),\n",
              " ('miss', 599),\n",
              " ('must', 566),\n",
              " ('harriet', 500),\n",
              " ('much', 484),\n",
              " ('said', 483),\n",
              " ('one', 447),\n",
              " ('weston', 437)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.1 One-hot Encoding\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `CountVectorizer`\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram ëª¨ë¸\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: 'Word2Vec, GloVe, FastText, CBOW / Skip-gram'\n",
        "\n",
        "#### 2.6 ì‚¬ì „í•™ìŠµ ì„ë² ë”©\n",
        "- ğŸ“Œ êµ¬í˜„ ë„êµ¬: Gensim / HuggingFace Transformers(word2vec-google-news-300)"
      ],
      "metadata": {
        "id": "Ki75EqrkxogK"
      },
      "id": "Ki75EqrkxogK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKO4QgTuvHeS"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "docs = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are amazing\",\n",
        "    \"I enjoy learning about embeddings\",\n",
        "    \"Embeddings capture semantic meaning\",\n",
        "    \"Natural language is complex\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(docs)\n",
        "print(\"BoW Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())"
      ],
      "id": "DKO4QgTuvHeS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q8LjfCvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(docs)\n",
        "print(\"TF-IDF Feature Names:\", tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ],
      "id": "I-Q8LjfCvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmcqoiuvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "bow_bigram = vectorizer_bigram.fit_transform(docs)\n",
        "print(\"Bi-gram Feature Names:\", vectorizer_bigram.get_feature_names_out())\n",
        "print(\"Bi-gram BoW Matrix:\\n\", bow_bigram.toarray())"
      ],
      "id": "XEmcqoiuvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpIVnr9xvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "tokenized_docs = [doc.lower().split() for doc in docs]\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=3, min_count=1, sg=1)\n",
        "print(\"Vector for 'language':\", w2v_model.wv['language'])\n",
        "print(\"Similarity between 'language' and 'natural':\", w2v_model.wv.similarity('language', 'natural'))\n",
        "print(\"Most similar to 'embeddings':\", w2v_model.wv.most_similar('embeddings', topn=3))"
      ],
      "id": "IpIVnr9xvHeU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqlWKH_HvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(model['computer'])\n",
        "print(model.most_similar(\"language\"))\n",
        "print(\"Similarity between 'language' and 'computer':\", model.similarity('language', 'computer'))"
      ],
      "id": "bqlWKH_HvHeU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}